{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "855a91d4-707c-48dd-9cc2-4fa6ca7c2e11",
   "metadata": {},
   "source": [
    "#### Food Recognition 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43169c6f-c597-4e86-955a-31d6496ec134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/sainikhileshreddy/food-recognition-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a12de2-702f-40c1-ba53-12378ca0b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import hub\n",
    "from glob import glob\n",
    "import cv2\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de1639-c831-4952-9bae-51d779247899",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = hub.load('autodl-tmp/hub/train')\n",
    "ds_val = hub.load('autodl-tmp/hub/val')\n",
    "def st(x):\n",
    "    file_name = x.split('/')[-1]\n",
    "    Id = file_name.split('.')[0]\n",
    "    return Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e129eb4-3f93-4614-b71a-0e448df4ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs = glob('autodl-tmp/raw_data/public_training_set_release_2.0/images/*')\n",
    "all_imgs.sort(key=st)\n",
    "all_cls = []\n",
    "for i in range(len(all_imgs)):\n",
    "    img = cv2.imread(all_imgs[i])\n",
    "    min_ = min(img.shape[:2])\n",
    "    max_ = max(img.shape[:2])\n",
    "    if min_>640:\n",
    "        img = A.CenterCrop(min_, min_)(image=img)['image']\n",
    "        img = cv2.resize(img, (640, 640), interpolation = cv2.INTER_AREA)\n",
    "    elif max_>640:\n",
    "        img = A.CenterCrop(min_, min_)(image=img)['image']\n",
    "    new_path = 'autodl-tmp/Food2022/'+all_imgs[i].split('/')[-1]\n",
    "    cv2.imwrite(new_path, img)\n",
    "    all_cls.append(ds_train['categories'][i].numpy()[0])\n",
    "df_dict = {'image_files': all_imgs,\n",
    "           'labels': all_cls}\n",
    "df = pd.DataFrame(df_dict)\n",
    "df.to_csv('autodl-tmp/food_first.csv', index=False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff34d74-25d3-4835-8416-9ccef4a17b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df.groupby('labels').count()\n",
    "(count>10).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed83ec-5e61-4c83-9d2f-d5963c88bc58",
   "metadata": {},
   "source": [
    "#### Stanford_Online_Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348e315-dd81-4080-8ceb-0039e09b86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cvgl.stanford.edu/projects/lifted_struct/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2505eb03-5be5-418c-a3d4-08cad24ffaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import hub\n",
    "from glob import glob\n",
    "import cv2\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b1597-ffbe-454c-abd0-2b5782d815d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs = glob('autodl-tmp/ori_data/Stanford_Products/*/*')\n",
    "calsses = []\n",
    "all_cls = []\n",
    "for file in tqdm(all_imgs):\n",
    "    cl_idx = file.split('/')[-1].split('_')[0]\n",
    "    try:\n",
    "        cl = classes.index(cl_idx)\n",
    "    except:\n",
    "        classes.append(cl_idx)\n",
    "        cl = classes.index(cl_idx)\n",
    "    all_cls.append(cl)\n",
    "df_dict = {'image_files': all_imgs,\n",
    "           'labels': all_cls}\n",
    "df = pd.DataFrame(df_dict)\n",
    "df.to_csv('autodl-tmp/ori_data/Stanford_Products/Stanford_Products.csv', index=False)\n",
    "print(df.nunique())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52a250-69b6-4e58-91a8-1ecc326bc14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df.groupby('labels').count()\n",
    "(count>10).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a7b966-17f3-4988-8211-4e1422d596a8",
   "metadata": {},
   "source": [
    "#### Fashion_200K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16dbb1-4370-4697-ace5-751c8dec58a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/mayukh18/fashion200k-dataset\n",
    "\n",
    "# !kaggle datasets download -d mayukh18/fashion200k-dataset -p autodl-tmp/ori_data/Fashion_200K\n",
    "# !unzip -o autodl-tmp/ori_data/Fashion_200K/fashion200k-dataset.zip -d autodl-tmp/ori_data/Fashion_200K/\n",
    "# !rm -rf autodl-tmp/ori_data/Fashion_200K/detection\n",
    "# !rm -rf autodl-tmp/ori_data/Fashion_200K/labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b0a9b0-d030-45e5-ac6f-02e83e970086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import hub\n",
    "from glob import glob\n",
    "import cv2\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2e331-764d-459b-b1cd-c4bce3c1256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = glob('autodl-tmp/ori_data/Fashion_200K/women/*/*/*')\n",
    "all_imgs = []\n",
    "all_cls  = []\n",
    "for i in range(len(classes)):\n",
    "    img_files = glob(os.path.join(classes[i], '*'))\n",
    "    all_cls.extend([i for _ in range(len(img_files))])\n",
    "    all_imgs.extend(img_files)\n",
    "df_dict = {'image_files': all_imgs,\n",
    "           'labels': all_cls}\n",
    "df = pd.DataFrame(df_dict)\n",
    "df.to_csv('autodl-tmp/ori_data/Fashion_200K/Fashion_200K.csv', index=False)\n",
    "print(df.nunique())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb6ad4-4e8c-4aa5-8bb1-c4a4753606e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df.groupby('labels').count()\n",
    "(count>10).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d49d5-15b4-4f60-871a-bb3ac6ba4a73",
   "metadata": {},
   "source": [
    "#### DeepFashion （Consumer-to-shop）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1d95df-1070-4067-a42f-e4face3c9dc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/sangamman/deepfashion-consumer-to-shop-training\n",
    "\n",
    "# !kaggle datasets download -d sangamman/deepfashion-consumer-to-shop-training -p autodl-tmp/ori_data/DeepFashion --unzip\n",
    "# !unzip -o autodl-tmp/ori_data/DeepFashion_CTS/deepfashion-consumer-to-shop-training.zip -d autodl-tmp/ori_data/DeepFashion_CTS/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a06d73-8bbc-4e7b-8062-23caf13a24f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import hub\n",
    "from glob import glob\n",
    "import cv2\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186d31a-2c75-428b-bf73-3389221a5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = glob('autodl-tmp/ori_data/DeepFashion_CTS/deepfashion/train/*/*')\n",
    "full_classes = list(map(lambda x: x.split('/')[-1].split('_')[1]), classes)\n",
    "classes = glob('autodl-tmp/ori_data/DeepFashion_CTS/deepfashion/*/*/*')\n",
    "all_imgs = []\n",
    "all_cls  = []\n",
    "for i in trange(len(classes)):\n",
    "    img_files = glob(os.path.join(classes[i], '*'))\n",
    "    try:\n",
    "        cl = full_classes.index(classes[i].split('/')[-1].split('_')[1])\n",
    "    except:\n",
    "        full_classes.append(classes[i].split('/')[-1].split('_')[1])\n",
    "        cl = full_classes.index(classes[i].split('/')[-1].split('_')[1])\n",
    "    all_cls.extend([cl for _ in range(len(img_files))])\n",
    "    all_imgs.extend(img_files)\n",
    "df_dict = {'image_files': all_imgs,\n",
    "           'labels': all_cls}\n",
    "df1 = pd.DataFrame(df_dict)\n",
    "print(df1.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b266d7e7-a82b-4514-b659-971366d1bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = glob('autodl-tmp/ori_data/DeepFashion_CTS/test/*/*')\n",
    "all_imgs = []\n",
    "all_cls  = []\n",
    "for i in trange(len(classes)):\n",
    "    img_files = glob(os.path.join(classes[i], '*', '*'))\n",
    "    try:\n",
    "        cl = full_classes.index(int(classes[i].split('/')[-1].split('_')[1]))\n",
    "    except:\n",
    "        full_classes.append(int(classes[i].split('/')[-1].split('_')[1]))\n",
    "        cl = full_classes.index(int(classes[i].split('/')[-1].split('_')[1]))\n",
    "    all_cls.extend([cl for _ in range(len(img_files))])\n",
    "    all_imgs.extend(img_files)\n",
    "df_dict = {'image_files': all_imgs,\n",
    "           'labels': all_cls}\n",
    "df2 = pd.DataFrame(df_dict)\n",
    "print(df2.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69fc2be-89c6-4b4e-9ccb-3355034a68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2], axis=0).reset_index(drop=True)\n",
    "df.to_csv('autodl-tmp/ori_data/DeepFashion_CTS/DeepFashion_CTS.csv', index=False)\n",
    "print(df.nunique())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5b52a-9817-4ecc-9fc5-5f5321ef140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df.groupby('labels').count()\n",
    "(count>10).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dbc7ea-7675-43d5-b4db-04e8b73c72db",
   "metadata": {},
   "source": [
    "#### Fruit （Grocery Store）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79a250-2366-4c1a-941a-5eed069b0176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/marcusklasson/GroceryStoreDataset\n",
    "\n",
    "# !unzip -o autodl-tmp/ori_data/Fruit/fruit.zip -d autodl-tmp/ori_data/Fruit/\n",
    "# !rm -rf autodl-tmp/ori_data/Fruit/iconic-images-and-descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a580dbb-9e9a-4886-9dc8-c072a24f432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import hub\n",
    "from glob import glob\n",
    "import cv2\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c1799b-d107-4b39-a23f-326e47c2358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = glob('autodl-tmp/ori_data/Fruit/train/*/*')\n",
    "full_classes = list(map(lambda x: x.split('/')[-1], classes))\n",
    "classes = glob('autodl-tmp/ori_data/Fruit/*/*/*')\n",
    "all_imgs = []\n",
    "all_cls  = []\n",
    "for i in trange(len(classes)):\n",
    "    img_files = glob(os.path.join(classes[i], '*'))\n",
    "    try:\n",
    "        cl = full_classes.index(classes[i].split('/')[-1])\n",
    "    except:\n",
    "        print(classes[i])\n",
    "        full_classes.append(classes[i].split('/')[-1])\n",
    "        cl = full_classes.index(classes[i].split('/')[-1])\n",
    "    all_cls.extend([cl for _ in range(len(img_files))])\n",
    "    all_imgs.extend(img_files)\n",
    "df_dict = {'image_files': all_imgs,\n",
    "           'labels': all_cls}\n",
    "df = pd.DataFrame(df_dict)\n",
    "df.to_csv('autodl-tmp/ori_data/Fruit/Fruit.csv', index=False)\n",
    "print(df.nunique())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d5125-2bad-4f43-af9c-38c6be1cc70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df.groupby('labels').count()\n",
    "(count>3).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e02d7a-980c-4ea7-a516-80bd5efeedec",
   "metadata": {},
   "source": [
    "#### Aliproducts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0736af6f-dab8-4a5f-8330-3fa0d3232c19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://tianchi.aliyun.com/competition/entrance/231780/introduction\n",
    "\n",
    "# !wget https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/231780/AliProducts/100001585554035/train_val.part1.tar.gz -P autodl-tmp/ori_data/Aliproducts/\n",
    "# !wget https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/231780/AliProducts/200001585540031/train_val.part2.tar.gz -P autodl-tmp/ori_data/Aliproducts/\n",
    "# !wget https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/231780/AliProducts/300001585559032/train_val.part3.tar.gz -P autodl-tmp/ori_data/Aliproducts/\n",
    "# !wget https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/231780/AliProducts/400001585578035/train_val.part4.tar.gz -P autodl-tmp/ori_data/Aliproducts/\n",
    "# !wget https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/231780/AliProducts/500001585599038/train_val.part5.tar.gz -P autodl-tmp/ori_data/Aliproducts/\n",
    "# !wget https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/231780/AliProducts/600001585536030/train_val.part6.tar.gz -P autodl-tmp/ori_data/Aliproducts/\n",
    "# !wget https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/231780/AliProducts/700001585524033/train_val.part7.tar.gz -P autodl-tmp/ori_data/Aliproducts/\n",
    "# !wget https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/231780/AliProducts/800001585502035/train_val.part8.tar.gz -P autodl-tmp/ori_data/Aliproducts/\n",
    "# !wget https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/231780/AliProducts/900001585552031/train_val.part9.tar.gz -P autodl-tmp/ori_data/Aliproducts/\n",
    "\n",
    "# !tar -zxvf autodl-tmp/ori_data/Aliproducts/train_val.part1.tar.gz -C autodl-tmp/ori_data/Aliproducts/train1\n",
    "# !tar -zxvf autodl-tmp/ori_data/Aliproducts/train_val.part2.tar.gz -C autodl-tmp/ori_data/Aliproducts/train2\n",
    "# !tar -zxvf autodl-tmp/ori_data/Aliproducts/train_val.part3.tar.gz -C autodl-tmp/ori_data/Aliproducts/train3\n",
    "# !tar -zxvf autodl-tmp/ori_data/Aliproducts/train_val.part4.tar.gz -C autodl-tmp/ori_data/Aliproducts/train4\n",
    "# !tar -zxvf autodl-tmp/ori_data/Aliproducts/train_val.part5.tar.gz -C autodl-tmp/ori_data/Aliproducts/train5\n",
    "# !tar -zxvf autodl-tmp/ori_data/Aliproducts/train_val.part6.tar.gz -C autodl-tmp/ori_data/Aliproducts/train6\n",
    "# !tar -zxvf autodl-tmp/ori_data/Aliproducts/train_val.part7.tar.gz -C autodl-tmp/ori_data/Aliproducts/train7\n",
    "# !tar -zxvf autodl-tmp/ori_data/Aliproducts/train_val.part8.tar.gz -C autodl-tmp/ori_data/Aliproducts/train8\n",
    "# !tar -zxvf autodl-tmp/ori_data/Aliproducts/train_val.part9.tar.gz -C autodl-tmp/ori_data/Aliproducts/train9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a9adb5-019f-43c8-8fd0-f7eddc0c8a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import hub\n",
    "from glob import glob\n",
    "import cv2\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6311953-71fc-42e7-ae5c-79f80f7bf997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_classes = glob('autodl-tmp/ori_data/Aliproducts/val/*')\n",
    "full_classes = list(map(lambda x: x.split('/')[-1], full_classes))\n",
    "classes = glob('autodl-tmp/ori_data/Aliproducts/*/*')\n",
    "all_imgs = []\n",
    "all_cls  = []\n",
    "for i in trange(len(classes)):\n",
    "    img_files = glob(os.path.join(classes[i], '*'))\n",
    "    try:\n",
    "        cl = full_classes.index(classes[i].split('/')[-1])\n",
    "    except:\n",
    "        print(classes[i])\n",
    "        full_classes.append(classes[i].split('/')[-1])\n",
    "        cl = full_classes.index(classes[i].split('/')[-1])\n",
    "    all_cls.extend([cl for _ in range(len(img_files))])\n",
    "    all_imgs.extend(img_files)\n",
    "df_dict = {'image_files': all_imgs,\n",
    "           'labels': all_cls}\n",
    "df = pd.DataFrame(df_dict)\n",
    "df.to_csv('autodl-tmp/ori_data/Aliproducts/Aliproducts.csv', index=False)\n",
    "print(df.nunique())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae24c708-8abb-47fc-b806-87f5f8a4ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df.groupby('labels').count()\n",
    "(count>3).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d3191-595f-442b-8940-46c82e7ab87e",
   "metadata": {},
   "source": [
    "#### Stanford_Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c480f2-619c-4027-9e62-ca1dade0a328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://ai.stanford.edu/~jkrause/cars/car_dataset.html\n",
    "\n",
    "#Stanford cars\n",
    "# !tar -zxvf autodl-tmp/cars_test.tgz -C autodl-tmp/ori_data/Stanford_Cars\n",
    "# !tar -zxvf autodl-tmp/cars_train.tgz -C autodl-tmp/ori_data/Stanford_Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c2000-f8eb-4068-a3f0-59e0f5280045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "from tqdm import tqdm, trange\n",
    "import cv2\n",
    "data_path=\"autodl-tmp/ori_data/Stanford_Cars/cars_annos.mat\"\n",
    "\n",
    "data = scio.loadmat(data_path, squeeze_me=True)\n",
    "all_imgs = []\n",
    "all_cls = []\n",
    "for anno in tqdm(data['annotations']):\n",
    "    all_imgs.append(anno[0])\n",
    "    all_cls.append(anno['class']-1)\n",
    "df_dict = {'image_files': all_imgs,\n",
    "           'labels': all_cls}\n",
    "df = pd.DataFrame(df_dict)\n",
    "df.to_csv('autodl-tmp/ori_data/Stanford_Cars/Stanford_Cars.csv', index=False)\n",
    "print(df.nunique())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d70e28-3454-4da2-9983-5c04468d8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df.groupby('labels').count()\n",
    "(count>3).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d041d85f-720c-47dd-9c39-6159af1b719d",
   "metadata": {},
   "source": [
    "#### Art_MET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1365ef-4ce3-446e-945e-96fbd8ef19fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/dschettler8845/the-met-dataset\n",
    "\n",
    "# !kaggle datasets download -d dschettler8845/the-met-dataset -p autodl-tmp/ori_data/Art_MET --unzip\n",
    "# !unzip -o /root/autodl-tmp/the-met-dataset.zip -d /root/autodl-tmp\n",
    "# !rm -rf autodl-tmp/ori_data/Art_MET/r18IN_con-synreal-closest\n",
    "# !rm -rf autodl-tmp/ori_data/Art_MET/r18SWSL_con-synreal-closest\n",
    "# !rm -rf autodl-tmp/ori_data/Art_MET/small_MET\n",
    "# !rm -rf autodl-tmp/ori_data/Art_MET/test_met\n",
    "# !rm -rf autodl-tmp/ori_data/Art_MET/test_noart\n",
    "# !rm -rf autodl-tmp/ori_data/Art_MET/test_other\n",
    "# !rm -rf autodl-tmp/ori_data/Art_MET/ground_truth\n",
    "# !rm -rf autodl-tmp/ori_data/Art_MET/descriptor_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e4079-7e27-41fe-a942-38caeb609e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "from glob import glob\n",
    "import cv2\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a0123a-30be-45cf-8f9f-f96c63ea8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = glob('autodl-tmp/ori_data/Art_MET/MET/*')\n",
    "all_imgs = []\n",
    "all_cls  = []\n",
    "for i in trange(len(classes)):\n",
    "    img_files = glob(os.path.join(classes[i], '*'))\n",
    "    all_cls.extend([i for _ in range(len(img_files))])\n",
    "    all_imgs.extend(img_files)\n",
    "df_dict = {'image_files': all_imgs,\n",
    "           'labels': all_cls}\n",
    "df = pd.DataFrame(df_dict)\n",
    "df.to_csv('autodl-tmp/ori_data/Art_MET/Art_MET.csv', index=False)\n",
    "print(df.nunique())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad1fe0-1b13-4e87-ba3a-3891ac0282a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('autodl-tmp/ori_data/Art_MET/Art_MET.csv')\n",
    "count = df.groupby('labels').count()\n",
    "(count>6).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5166515-289c-4512-9477-2231048503ee",
   "metadata": {},
   "source": [
    "#### DeepFashion2（hard-triplets）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9bc844-82ae-4e22-9159-85727212af8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/sangamman/deepfashion2-hard-triplets\n",
    "\n",
    "# !kaggle datasets download -d sangamman/deepfashion2-hard-triplets -p autodl-tmp/ori_data/DeepFashion2 --unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2220f9fb-9087-4b4b-8a5c-17334e553bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "from glob import glob\n",
    "import cv2\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c282c8d-84a2-4e4a-b845-2b3a63956840",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = glob('autodl-tmp/ori_data/DeepFashion2/*/*/*')\n",
    "full_classes = glob('autodl-tmp/ori_data/DeepFashion2/fasion_dataset_similar_pair_croped_train/*/*')\n",
    "full_classes = list(map(lambda x: x.split('/')[-1], full_classes))\n",
    "all_imgs = []\n",
    "all_cls  = []\n",
    "for i in trange(len(classes)):\n",
    "    img_files = glob(os.path.join(classes[i], '*/*'))\n",
    "    try:\n",
    "        cl = full_classes.index(classes[i].split('/')[-1])\n",
    "    except:\n",
    "        full_classes.append(classes[i].split('/')[-1])\n",
    "        cl = full_classes.index(classes[i].split('/')[-1])\n",
    "    all_cls.extend([cl for _ in range(len(img_files))])\n",
    "    all_imgs.extend(img_files)\n",
    "\n",
    "df_dict = {'image_files': all_imgs,\n",
    "           'labels': all_cls}\n",
    "df = pd.DataFrame(df_dict)\n",
    "df.to_csv('autodl-tmp/ori_data/DeepFashion2/DeepFashion2.csv', index=False)\n",
    "print(df.nunique())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bca3af-e909-4bc4-9750-7330c645766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df.groupby('labels').count()\n",
    "(count>3).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c824274a-4ab3-4585-a604-8b132e50e797",
   "metadata": {},
   "source": [
    "### RP2K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2926ae65-d90e-46e2-bd8a-39909578fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.pinlandata.com/rp2k_dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f93cc-92e5-4b5a-97a7-0b045da504d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "from glob import glob\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69975402-95fc-4941-ab55-ee426e722113",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_classes = glob('autodl-tmp/ori_data/rp2k/train/*')\n",
    "full_classes = list(map(lambda x: x.split('/')[-1], full_classes))\n",
    "classes = glob('autodl-tmp/ori_data/rp2k/*/*')\n",
    "all_imgs = []\n",
    "all_cls  = []\n",
    "for i in trange(len(classes)):\n",
    "    img_files = glob(os.path.join(classes[i], '*'))\n",
    "    try:\n",
    "        cl = full_classes.index(classes[i].split('/')[-1])\n",
    "    except:\n",
    "        print(classes[i])\n",
    "        full_classes.append(classes[i].split('/')[-1])\n",
    "        cl = full_classes.index(classes[i].split('/')[-1])\n",
    "    all_cls.extend([cl for _ in range(len(img_files))])\n",
    "    all_imgs.extend(img_files)\n",
    "df_dict = {'image_files': all_imgs,\n",
    "           'labels': all_cls}\n",
    "df = pd.DataFrame(df_dict)\n",
    "df.to_csv('autodl-tmp/ori_data/rp2k/rp2k.csv', index=False)\n",
    "print(df.nunique())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2edd7b-1376-485e-ba1a-6ca208e2fc4b",
   "metadata": {},
   "source": [
    "### Food2021 （largefinefoodai-iccv-recognition）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6084c364-bfd0-4fb5-8b38-244b6af14d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/competitions/largefinefoodai-iccv-recognition/data\n",
    "\n",
    "# !wget https://s3plus.meituan.net/v1/mss_fad1a48f61e8451b8172ba5abfdbbee5/foodai-workshop-challenge/Train.tar -P autodl-tmp/\n",
    "# !tar -xvf autodl-tmp/Train.tar -C autodl-tmp/ori_data/Food2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8516c4-21ba-4bb6-9e53-4df2b28a6083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "from glob import glob\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d45ec5c-7482-4308-8013-9907d7ab7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = glob('autodl-tmp/ori_data/Food2021/*')\n",
    "all_imgs = []\n",
    "all_cls  = []\n",
    "for i in trange(len(classes)):\n",
    "    img_files = glob(os.path.join(classes[i], '*'))\n",
    "    all_cls.extend([i for _ in range(len(img_files))])\n",
    "    all_imgs.extend(img_files)\n",
    "df_dict = {'image_files': all_imgs,\n",
    "           'labels': all_cls}\n",
    "df = pd.DataFrame(df_dict)\n",
    "df.to_csv('autodl-tmp/ori_data/Food2021/Food2021.csv', index=False)\n",
    "print(df.nunique())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa5acd9-31b7-403f-965c-c0f72a5c668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df.groupby('labels').count()\n",
    "(count>3).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcfe667-ad21-4125-801e-1c005b3709cd",
   "metadata": {},
   "source": [
    "### Landmark2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eedeef6-e282-4d40-aa87-b69c3fc5bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/competitions/landmark-retrieval-2021\n",
    "\n",
    "# !kaggle competitions download -c landmark-retrieval-2021 -p autodl-tmp/ori_data/Landmark2021\n",
    "# !unzip -o autodl-tmp/ori_data/Landmark2021/landmark-retrieval-2021.zip -d autodl-tmp/ori_data/Landmark2021/\n",
    "# !rm -rf autodl-tmp/ori_data/Landmark2021/index\n",
    "# !rm -rf autodl-tmp/ori_data/Landmark2021/test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9bcce-b3b8-49fd-9023-4603598a2ed6",
   "metadata": {},
   "source": [
    "### Shopee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d6c296-984e-419f-8c0d-5a42c9a61317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/competitions/shopee-product-matching\n",
    "\n",
    "# !kaggle competitions download -c shopee-product-matching -p autodl-tmp/ori_data/Shopee\n",
    "# !unzip -o autodl-tmp/ori_data/Shopee/shopee-product-matching.zip -d autodl-tmp/ori_data/Shopee/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e2cda4-b88d-4e43-bca9-12e0497240e4",
   "metadata": {},
   "source": [
    "### JD-product-10K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a0d482-7717-4773-b980-201e31be0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/products-10k\n",
    "\n",
    "# !unzip -o autodl-tmp/JD-product10K.zip -d autodl-tmp/ori_data/JD_Products_10K"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
